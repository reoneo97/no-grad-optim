{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h1><center>DSA 4212: Year 2021</center></h1>\n",
    "<h3><center> Assignment 1 (Deadline: 28 March 2021 at 23:59) </center></h3>\n",
    "<h3><center> To Be submitted on the lumiNUS )</center></h3>\n",
    "<h2><center> Group Number: 32 </center></h2>\n",
    "<h2><center> Filename: assignment_1_XXXX.ipynb where XXX is your group number</center></h2>\n",
    "<h2><center> Group Member 1: Neo Zheng Jie Reo, A0183286W </center></h2>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this assignment is to explore 3 optimization methods that do NOT use any derivative information: \n",
    "1. **Particle swarm optimization**: https://en.wikipedia.org/wiki/Particle_swarm_optimization\n",
    "2. **Simulated annealing**: https://en.wikipedia.org/wiki/Simulated_annealing\n",
    "3. **Nelder-Mead**: https://en.wikipedia.org/wiki/Nelderâ€“Mead_method\n",
    "\n",
    "Start to read about these methods on wikipedia, as well as any other source [books / articles / blogs / etc...] that you seem appropriate. Describe as clearly as possible how these methods work and implement then from scratch. Explore the importance of the several tuning parameters. Discuss the advantage and drawbacks of these classes of methods and test them on some of the examples presented here: https://en.wikipedia.org/wiki/Test_functions_for_optimization\n",
    "\n",
    "\n",
    "**Remark:**\n",
    "1. acknowledge **all** the sources you are using to create this report. \n",
    "2. the clarity and readability of the report is important.\n",
    "3. read widely and experiment.\n",
    "4. It may be interesting/useful to create animations: the python package Celluloid is particularly easy to use [there are many others]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Gradient-based optimization methods are widely used in the field of machine learning today. With the backpropagation algorithm and use of computational graphs, gradient-based methods have become highly scalable and exclusively used to train deep networks which may contain billions of parameters. Gradients allow us to understand the local topology of the function that is being optimized but is not always available. Many problems are inherently discrete in nature and cannot be easily expressed as smooth and continuous functions. This report discusses three different optimization methods that do not require the use of gradients in the optimization procedure. In addition to understanding the mechanics and Python Implementation, they will also be evaluated on optimization test functions to evaluate their ability to find the speed, convergence rate and ability to find the global minimum\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particle Swarm Optimisation\n",
    "\n",
    "\n",
    "Particle Swarm Optimization (PSO) is a heuristic where the search space is populated by a large number of \"particles\" which are essentially initial guesses for where the global minimum might be. These particles are initialized with a uniformly distributed position and velocity over the entire search space. At each iteration, the velocity of the particles can be adjusted based on:\n",
    "1. The optimal location which the particle has visited\n",
    "2. The optimal location which has been visited by another particle in the swarm\n",
    "\n",
    "By utilising the \"wisdom of the crowd\", the swarm should ideally converge towards the global optima of the function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Implementation\n",
    "\n",
    "For the python implementation of the three algorithms, the code will follow a similar structure. Firstly, the algorithm will be instantiated as a Python Class and contain the `optimize` method. This optimize method given the \n",
    "Since all the particles behave identically, the particles are best represented as an instance of a Python Class. There are three important parameters which will determine the behavior of the algorithm. These parameters are:\n",
    "\n",
    "1. Learning Rate - How much does the velocity of the particle affect its movement\n",
    "2. $\\phi_g$ - How much is the velocity affected by the \n",
    "3. $\\phi_p$\n",
    "\n",
    "\n",
    "Lastly, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSO():\n",
    "    def __init__(self,l_r,w,p_g,p_p,max_iter):\n",
    "        self.l_r = l_r\n",
    "        self.w = w\n",
    "        self.p_g = p_g\n",
    "        self.p_p = p_p\n",
    "        self.max_iter = max_iter\n",
    "    def optimize(self,f,lbs,ubs):\n",
    "        \n",
    "        \n",
    "class Particle():\n",
    "    def __init__(self,position,velocity,**params):\n",
    "        self.position = position\n",
    "        self.velocity = velocity \n",
    "        self.optima = optima\n",
    "        \n",
    "    def move(self,swarm_optima):\n",
    "        pass\n",
    "    def eval_current_position(self,f):\n",
    "        return f(self.position)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulated Annealing\n",
    "\n",
    "Simulated Annealing (SA) is a probabilistic approach to an optimization problem. SA only finds an approximate global optimum for a function and is extremely effective at solving combinatorial or discrete optimization problems. The SA algorithm is inspired by annealing in metallurgy where successive heating and cooling allows will ultimately produce the best result. Similarly the algorithm functions by varying a `temperature` parameter throughout the process. The `temperature` parameter determines how likely it is for the algorithm to choose a low probability solution, high `temperature` allows the algorithm to explore the search space more and as the `temperature` cools\n",
    "\n",
    "As mentioned earlier, SA is best utilized for combinatorial or discrete problems and thus alterations must be made to use SA for continuous functions. One important thing for SA to function is to have neighbouring states which allows the functions to move "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimulatedAnnealing():\n",
    "    def __init__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nelder-Mead Method\n",
    "The Nelder-Mead method is a heuristic search method which uses a simplex as the search area to slowly converge onto the optimum value. Mathematically, a simplex is a $n$ dimensional object which consists of having $n+1$ vertices. The method initializes $n+1$ test points around the search function which is arranged in a simplex. By evaluating the function at the $n+1$ test points, the function can be approximated and extrapolated to derive a new test point with a lower value. This more optimal point will then replace the worst point in the simplex and this process is repeated until convergence is met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
